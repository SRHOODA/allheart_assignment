# response = requests.get('https://www.google.com/maps/search/resturants+in+karnal')  
**above url did not work

# response = requests.get(f'https://www.google.com/maps/search/dentists+in+delhi/@29.6904024,76.9922852,14z/data=!3m1!4b1') 
**above url did not work as expected, so used below one for this assignment

response = requests.get('https://www.google.com/search?q=dentists+in+karnal')


***i have tried every possible way on my side but above urls did not work.

soup = BeautifulSoup(response.text, 'html.parser')

# search_results = soup.select('.siAUzd-neVct section-scrollbox cYB2Ge-oHo7ed cYB2Ge-ti6hGc siAUzd-neVct-Q3DXx-BvBYQ siAUzd-neVct-YbohUe-bnBfGc')
# search_results = soup.select('.XS5mzc-on-hJDwNd-uMX1Ee siAUzd-neVct')
# search_results = soup.select('.siAUzd-neVct siAUzd-neVct-Q3DXx-horizontal')
# search_results = soup.select('.V0h1Ob-haAclf gd9aEe-LQLjdd OPZbO-KE6vqe')
# search_results = soup.select('.aopO7e-clz4Ic aopO7e-clz4Ic-bottom-RWgCYc')
# search_results = soup.select('.TFQHme')
# search_results = soup.select('.siAUzd-neVct siAUzd-neVct-H9tDt')
# search_results = soup.select('.siAUzd-neVct section-scrollbox cYB2Ge-oHo7ed cYB2Ge-ti6hGc siAUzd-neVct-Q3DXx-BvBYQ')
# search_results = soup.select('.a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd')
# search_results = soup.select('.MVVflb-haAclf V0h1Ob-haAclf-d6wfac MVVflb-haAclf-uxVfW-hSRGPd')
# search_results = soup.select('.Yr7JMd-pane-content cYB2Ge-oHo7ed')
# search_results = soup.select('.Yr7JMd-pane-content-ZYyEqf')
# search_results = soup.select('.CUwbzc-content gm2-body-2')

# search_results = soup.find('div', attrs = {'class':'CUwbzc-content gm2-body-2'}) 
# search_results = soup.find('div', attrs = {'class':'Yr7JMd-pane-content-ZYyEqf'}) 
# search_results = soup.find('div', attrs = {'class':'Yr7JMd-pane-content cYB2Ge-oHo7ed'}) 
# search_results = soup.find('div', attrs = {'class':'keynav-mode-off screen-mode'}) 
# search_results = soup.find('div', attrs = {'class':'Yr7JMd-pane Yr7JMd-pane-visible'}) 
# search_results = soup.find('div', attrs = {'class':'dbg0pd OSrXXb'}) 
search_results = soup.find('div', attrs = {'class':'SMWA9c'}) 

# search_results = soup.title
# search_results = soup.p
# search_results = soup.find_all('a')

# for link in soup.find_all('a'):
#     print(link.get('href'))

# print(soup.get_text())
# print(soup.attrs)

# with open('checking.html','w') as f:
#     f.write(soup.prettify())
#     # f.write(soup.text)
#     print('SUCCESS')



####-------(moved inside try block)adding data to the table-------------------
# sql_for_table = '''
# create table places_and_location_info(
# sno int primary key auto_increment, 
# name text,
# phone_number text,
# address MEDIUMTEXT,
# website text);
# '''

# for item in range(len(record)):

#     sql = "INSERT INTO places_and_location_info (name, phone_number, address, website) VALUES (%s, %s)"
#     val = (item[0], item[1], item[2], item[3])
#     my_cursor.execute(sql, val)
#     db.commit()

# print('saved sucessfully')

##*To deal with latin letters, i have also used this query as well
# ALTER TABLE python.places_and_location_info MODIFY COLUMN name, address MEDIUMTEXT  
# CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL;


##to check if data is saved properly.
# def checking():
#     my_cursor.execute('select * from places_and_location_info')
#     result = my_cursor.fetchall()
#     return result

# output = checking()
# for t_data in output:
#     print(t_data)



'''
Rough Work of selenium file

# from selenium import webdriver
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.action_chains import ActionChains
# import time

# class WebDriver:

# 	location_data = {}

# 	def __init__(self):
# 		self.PATH = "/home/sahil/chromedriver"
# 		self.options = Options()
# 		# self.options.binary_location = "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe"
# 		self.options.add_argument("--headless")
# 		self.driver = webdriver.Chrome(self.PATH, options=self.options)

# 		self.location_data["rating"] = "NA"
# 		self.location_data["reviews_count"] = "NA"
# 		self.location_data["location"] = "NA"
# 		self.location_data["contact"] = "NA"
# 		self.location_data["website"] = "NA"
# 		self.location_data["Time"] = {"Monday":"NA", "Tuesday":"NA", "Wednesday":"NA", "Thursday":"NA", "Friday":"NA", "Saturday":"NA", "Sunday":"NA"}
# 		self.location_data["Reviews"] = []
# 		self.location_data["Popular Times"] = {"Monday":[], "Tuesday":[], "Wednesday":[], "Thursday":[], "Friday":[], "Saturday":[], "Sunday":[]}

# 	def click_open_close_time(self):

# 		if(len(list(self.driver.find_elements_by_class_name("cX2WmPgCkHi__section-info-hour-text")))!=0):
# 			element = self.driver.find_element_by_class_name("cX2WmPgCkHi__section-info-hour-text")
# 			self.driver.implicitly_wait(5)
# 			ActionChains(self.driver).move_to_element(element).click(element).perform()

# 	def click_all_reviews_button(self):

# 		try:
# 			WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, "allxGeDnJMl__button")))

# 			element = self.driver.find_element_by_class_name("allxGeDnJMl__button")
# 			element.click()
# 		except:
# 			self.driver.quit()
# 			return False

# 		return True

# 	def get_location_data(self):

# 		try:
# 			# avg_rating = self.driver.find_element_by_class_name("section-star-display")
# 			avg_rating = self.driver.find_element_by_class_name("aMPvhf-fI6EEc-KVuj8d")
# 			# total_reviews = self.driver.find_element_by_class_name("section-rating-term")
# 			total_reviews = self.driver.find_element_by_class_name("widget-pane-link")
# 			# address = self.driver.find_element_by_css_selector("[data-item-id='address']")
# 			address = self.driver.find_element_by_css_selector("QSFF4-text'")[0]
# 			# phone_number = self.driver.find_element_by_css_selector("[data-tooltip='Copy phone number']")
# 			phone_number = self.driver.find_element_by_css_selector("QSFF4-text")[1]
# 			# website = self.driver.find_element_by_css_selector("[data-item-id='authority']")
# 			website = self.driver.find_element_by_css_selector("QSFF4-text")[2]

# 		except:
# 			pass
# 		try:
# 			self.location_data["rating"] = avg_rating.text
# 			self.location_data["reviews_count"] = total_reviews.text[1:]
# 			self.location_data["location"] = address.text
# 			self.location_data["contact"] = phone_number.text
# 			self.location_data["website"] = website.text
            

            

# 		except:
# 			pass


# 	def get_location_open_close_time(self):

# 		try:
# 			days = self.driver.find_elements_by_class_name("lo7U087hsMA__row-header")
# 			times = self.driver.find_elements_by_class_name("lo7U087hsMA__row-interval")

# 			day = [a.text for a in days]
# 			open_close_time = [a.text for a in times]

# 			for i, j in zip(day, open_close_time):
# 				self.location_data["Time"][i] = j
		
# 		except:
# 			pass

# 	def get_popular_times(self):
# 		try:
# 			a = self.driver.find_elements_by_class_name("section-popular-times-graph")
# 			dic = {0:"Sunday", 1:"Monday", 2:"Tuesday", 3:"Wednesday", 4:"Thursday", 5:"Friday", 6:"Saturday"}
# 			l = {"Sunday":[], "Monday":[], "Tuesday":[], "Wednesday":[], "Thursday":[], "Friday":[], "Saturday":[]}
# 			count = 0

# 			for i in a:
# 				b = i.find_elements_by_class_name("section-popular-times-bar")
# 				for j in b:
# 					x = j.get_attribute("aria-label")
# 					l[dic[count]].append(x)
# 				count = count + 1

# 			for i, j in l.items():
# 				self.location_data["Popular Times"][i] = j
# 		except:
# 			pass

# 	def scroll_the_page(self):
# 		try:
# 			WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "section-layout-root")))

# 			pause_time = 2
# 			max_count = 5
# 			x = 0

# 			while(x<max_count):
# 				scrollable_div = self.driver.find_element_by_css_selector('div.section-layout.section-scrollbox.scrollable-y.scrollable-show')
# 				try:
# 					self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)
# 				except:
# 					pass
# 				time.sleep(pause_time)
# 				x=x+1
# 		except:
# 			self.driver.quit()

# 	def expand_all_reviews(self):
# 		try:
# 			element = self.driver.find_elements_by_class_name("section-expand-review")
# 			for i in element:
# 				i.click()
# 		except:
# 			pass

# 	def get_reviews_data(self):
# 		try:
# 			review_names = self.driver.find_elements_by_class_name("section-review-title")
# 			review_text = self.driver.find_elements_by_class_name("section-review-review-content")
# 			review_dates = self.driver.find_elements_by_css_selector("[class='section-review-publish-date']")
# 			review_stars = self.driver.find_elements_by_css_selector("[class='section-review-stars']")

# 			review_stars_final = []

# 			for i in review_stars:
# 				review_stars_final.append(i.get_attribute("aria-label"))

# 			review_names_list = [a.text for a in review_names]
# 			review_text_list = [a.text for a in review_text]
# 			review_dates_list = [a.text for a in review_dates]
# 			review_stars_list = [a for a in review_stars_final]

# 			for (a,b,c,d) in zip(review_names_list, review_text_list, review_dates_list, review_stars_list):
# 				self.location_data["Reviews"].append({"name":a, "review":b, "date":c, "rating":d})

# 		except Exception as e:
# 			pass

# 	def scrape(self, url):
# 		try:
# 			self.driver.get(url)
# 		except Exception as e:
# 			self.driver.quit()
# 			return
# 		time.sleep(10)

# 		self.click_open_close_time()
# 		self.get_location_data()
# 		self.get_location_open_close_time()
# 		self.get_popular_times()
# 		if(self.click_all_reviews_button()==False):
# 			return(self.location_data)
# 		time.sleep(5)
# 		self.scroll_the_page()
# 		self.expand_all_reviews()
# 		self.get_reviews_data()
# 		self.driver.quit()

# 		return(self.location_data)


# url = "https://www.google.com/maps/search/dentists+in+karnal/@28.6119787,77.1324426,12z"
# x = WebDriver()
# print(x.scrape(url))



####another ways
# from selenium import webdriver
# from pprint import pprint


# chromedrive_path = '/home/sahil/chromedriver' # use the path to the driver you downloaded from previous steps
# driver = webdriver.Chrome(chromedrive_path)

# url = "https://www.google.com/maps/search/dentists+in+karnal/@28.6119787,77.1324426,12z"
# driver.get(url)

# page_content = driver.page_source   
# # pprint(page_content)
# with open('checking.html','w') as f:
#     f.write(page_content)


# from parsel import Selector
# from pprint import pprint

# response = Selector(page_content)
# results = []

# for el in response.xpath('//div[contains(@aria-label, "Results for")]/div/div[./a]'):
#     results.append({
#         'link': el.xpath('./a/@href').extract_first(''),
#         'title': el.xpath('./a/@aria-label').extract_first(''),
#     })
# #     # pprint(el)

# pprint(results)

# driver.quit()



#####last way using selenium
#Import neccesary libraries

# from selenium import webdriver              # driver to control chrome browser
# # import pyautogui                            
# from bs4 import BeautifulSoup          # to parse the html code
# # import threading                       # to do multi threding
# import time 
# import pandas as pd                   # to store data in csv file


# #enter the filename

# print("Enter the filename")            # filename to store data
# filename=str(input())


# #intiate the chrome webdriver instance
# browser=webdriver.Chrome()           # chrome instance 
# record=[]
# e=[]
# def Selenium_extractor():
#     time.sleep(5)
#     a=browser.find_elements_by_class_name("VkpGBb")
#     time.sleep(5)
#     for i in range(len(a)):
#         a[i].click()
#         time.sleep(5)
#         source=browser.page_source

#         #Beautiful soup for scraping the html source
#         soup=BeautifulSoup(source,'html.parser')
#         try:
#             Name_Html=soup.findAll('div',{"class":"SPZz6b"})
            
#             name=Name_Html[0].text
#             if name not in e:
#                 e.append(name)
#                 print(name)
#                 Phone_Html=soup.findAll('span',{"class":"LrzXr zdqRlf kno-fv"})    
#                 phone=Phone_Html[0].text
#                 print(phone)
            
#                 Address_Html=soup.findAll('span',{"class":"LrzXr"})
                
#                 address=Address_Html[0].text
#                 #print(address)
#                 try:
#                     Website_Html=soup.findAll('div',{"class":"QqG1Sd"})
#                     web=Website_Html[0].findAll('a')
            
#                     website=web[0].get('href')
#                 except:
#                     website="Not available"
#                 #print(website)
#                 record.append((name,phone,address,website))
#                 df=pd.DataFrame(record,columns=['Name','Phone number','Address','Website'])  # writing data to the file
#                 df.to_csv(filename + '.csv',index=False,encoding='utf-8')

#         except:
#             print("error")
#             continue
            

        
#     try:
#         time.sleep(5)            
#         next_button=browser.find_element_by_id("pnnext")                  # clicking the next button
#         next_button.click()
#         browser.implicitly_wait(10)
#         time.sleep(5)
#         Selenium_extractor()
#     except:
#         print("ERROR occured at clicking next button")
        

# print("Enter the link of the page")
# link=input()
# browser.get(str(link)) 
# time.sleep(10)                                             # pausing the program for 10 secs
# Selenium_extractor()

'''